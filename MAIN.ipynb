{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790b191d-0fa1-4f79-b810-6a9e04cf39ac",
   "metadata": {},
   "source": [
    "# Downloading all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622458e1-e0f2-46c5-97dc-95a08849ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Install all required Python packages ---\n",
    "# The '!' allows us to run shell commands directly from the notebook.\n",
    "# We are installing all the libraries identified in the import statements.\n",
    "!pip install pandas numpy beautifulsoup4 emoji nltk spacy tqdm textblob scikit-learn distance fuzzywuzzy python-Levenshtein\n",
    "\n",
    "# --- Step 2: Download necessary NLTK data ---\n",
    "# This command downloads the 'stopwords' corpus used for text cleaning.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# --- Step 3: Download the SpaCy language model ---\n",
    "# This downloads the small English model required for lemmatization.\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "print(\"\\n✅ All dependencies have been installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc3f57-750c-4992-acd3-fadb06061d07",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40ce20-e54f-4b32-bf38-ad61436451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52a792-51d5-4b2b-877b-197c01e7aa91",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8e4d4-8da2-4223-983e-572932407855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "df = pd.read_csv('Dataset/train.csv')\n",
    "\n",
    "#Shortening the Dataset\n",
    "df = df.head(30)\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "column_names = [\"question1\", \"question2\"]   # Put the LIST of column name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d450c5-6ccf-4058-b00a-e65e537f4720",
   "metadata": {},
   "source": [
    "# Preprocessing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4993d75-29fd-41b0-a747-b040a173ea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_data(df, column_names,\n",
    "                         # --- General Cleaning ---\n",
    "                         lower_case=True,\n",
    "                         remove_html=True,\n",
    "                         remove_urls=True,\n",
    "                         # --- Word & Character Handling ---\n",
    "                         replace_special_chars=True,\n",
    "                         decontract_words=True,\n",
    "                         chat_word_treatment=True, # This parameter is present but not explicitly used in the provided code.\n",
    "                         handle_emojis='replace', # Options: 'replace' (with text) or 'remove'\n",
    "                         # --- Advanced NLP ---\n",
    "                         remove_punc=True,\n",
    "                         remove_stopwords=True,\n",
    "                         spell_correction=False, # Disabled by default due to slowness\n",
    "                         root_word_reduction='lemmatize'): # Options: 'lemmatize', 'stem', or None\n",
    "    \"\"\"\n",
    "    Applies a full suite of text preprocessing steps to specified DataFrame columns\n",
    "    with progress bars for each step.\n",
    "\n",
    "    Parameters are togglable to control the preprocessing pipeline.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column_names (list): A list of column names in the DataFrame to preprocess.\n",
    "        lower_case (bool, optional): If True, converts text to lowercase. Defaults to True.\n",
    "        remove_html (bool, optional): If True, removes HTML tags from text. Defaults to True.\n",
    "        remove_urls (bool, optional): If True, removes URLs from text. Defaults to True.\n",
    "        replace_special_chars (bool, optional): If True, replaces common special characters\n",
    "                                                  and number shorthands. Defaults to True.\n",
    "        decontract_words (bool, optional): If True, expands contractions (e.g., \"don't\" to \"do not\"). Defaults to True.\n",
    "        chat_word_treatment (bool, optional): Placeholder for chat word treatment. Not implemented in provided code. Defaults to True.\n",
    "        handle_emojis (str, optional): Strategy for handling emojis. 'replace' converts emojis\n",
    "                                       to their text description, 'remove' removes them. Defaults to 'replace'.\n",
    "        remove_punc (bool, optional): If True, removes punctuation from text. Defaults to True.\n",
    "        remove_stopwords (bool, optional): If True, removes common English stopwords. Defaults to True.\n",
    "        spell_correction (bool, optional): If True, applies spell correction using TextBlob.\n",
    "                                           Can be slow for large datasets. Defaults to False.\n",
    "        root_word_reduction (str, optional): Method for reducing words to their root form.\n",
    "                                             'lemmatize' uses lemmatization (requires spaCy's 'en_core_web_sm' model),\n",
    "                                             'stem' uses Porter Stemmer, None skips this step. Defaults to 'lemmatize'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the specified text columns preprocessed.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    # Initialize tqdm for pandas to show progress bars for apply operations\n",
    "    tqdm.pandas(desc=\"Overall Progress\")\n",
    "\n",
    "    # --- Helper Functions with Integrated Logic ---\n",
    "\n",
    "    def _remove_html_tags(text):\n",
    "        \"\"\"Removes HTML tags from a given text using BeautifulSoup.\"\"\"\n",
    "        from bs4 import BeautifulSoup\n",
    "        return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    def _remove_url(text):\n",
    "        \"\"\"Removes URLs from a given text using regular expressions.\"\"\"\n",
    "        import re\n",
    "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    def _replace_special_chars(text):\n",
    "        \"\"\"\n",
    "        Replaces common special characters and expands number shorthands (e.g., '1000' to '1k').\n",
    "        \"\"\"\n",
    "        import re\n",
    "        text = text.replace('%', ' percent')\n",
    "        text = text.replace('$', ' dollar ')\n",
    "        text = text.replace('₹', ' rupee ')\n",
    "        text = text.replace('€', ' euro ')\n",
    "        text = text.replace('@', ' at ')\n",
    "        text = text.replace('[math]', '') # Specific pattern removal for '[math]'\n",
    "\n",
    "        # Number shorthands: e.g., 1,000,000,000 -> 1b\n",
    "        text = text.replace(',000,000,000 ', 'b ')\n",
    "        text = text.replace(',000,000 ', 'm ')\n",
    "        text = text.replace(',000 ', 'k ')\n",
    "        # Regex for numbers followed by zeros to apply shorthands\n",
    "        text = re.sub(r'([0-9]+)000000000', r'\\1b', text)\n",
    "        text = re.sub(r'([0-9]+)000000', r'\\1m', text)\n",
    "        text = re.sub(r'([0-9]+)000', r'\\1k', text)\n",
    "        return text\n",
    "\n",
    "    # Dictionary containing common English contractions and their expanded forms\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"can not\", \"can't've\": \"can not have\",\n",
    "        \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
    "        \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\", \"so's\": \"so as\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    def _decontract_words(text):\n",
    "        \"\"\"Expands contractions in a given text based on the `contractions` dictionary.\"\"\"\n",
    "        decontracted_text = []\n",
    "        for word in text.split():\n",
    "            if word in contractions:\n",
    "                decontracted_text.append(contractions[word])\n",
    "            else:\n",
    "                decontracted_text.append(word)\n",
    "        return ' '.join(decontracted_text)\n",
    "\n",
    "    # --- The rest of the helper functions from the original script ---\n",
    "    import emoji\n",
    "    import re\n",
    "    import string\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    import spacy # Assuming spaCy is installed and 'en_core_web_sm' model is downloaded\n",
    "\n",
    "    # Load the English spaCy model for lemmatization\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except OSError:\n",
    "        print(\"SpaCy model 'en_core_web_sm' not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "        # Fallback or raise an error depending on desired behavior\n",
    "        nlp = None\n",
    "\n",
    "    _demojize_text = lambda text: emoji.demojize(text) # Converts emojis to their textual representation\n",
    "    def _remove_emoji(text):\n",
    "        \"\"\"Removes all emojis from a given text using a regex pattern.\"\"\"\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    _remove_punc = lambda text: text.translate(str.maketrans('', '', string.punctuation)) # Removes all punctuation\n",
    "    english_stopwords = set(stopwords.words('english')) # Set of common English stopwords for efficient lookup\n",
    "    _remove_stopwords = lambda text: \" \".join([word for word in text.split() if word not in english_stopwords]) # Removes stopwords\n",
    "    ps = PorterStemmer() # Initialize Porter Stemmer\n",
    "    _stem_words = lambda text: \" \".join([ps.stem(word) for word in text.split()]) # Stems words\n",
    "    _lemmatize_text = lambda text: \" \".join([token.lemma_ for token in nlp(text)]) if nlp else text # Lemmatizes words using spaCy\n",
    "\n",
    "    # --- Applying Preprocessing Steps Sequentially with Progress Bars ---\n",
    "    for col in column_names:\n",
    "        print(f\"\\n--- Processing Column: {col} ---\")\n",
    "        # Ensure the column is of string type before applying text operations\n",
    "        processed_df[col] = processed_df[col].astype(str)\n",
    "\n",
    "        if remove_html:\n",
    "            # Apply HTML tag removal with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_remove_html_tags)\n",
    "        if remove_urls:\n",
    "            # Apply URL removal with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_remove_url)\n",
    "        if lower_case:\n",
    "            # Convert text to lowercase\n",
    "            processed_df[col] = processed_df[col].str.lower()\n",
    "        if replace_special_chars:\n",
    "            # Apply special character replacement with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_replace_special_chars)\n",
    "        if decontract_words:\n",
    "            # Apply word decontraction with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_decontract_words)\n",
    "\n",
    "        # Chat word treatment can be added here if needed, it overlaps with decontraction\n",
    "        # The 'chat_word_treatment' parameter is defined but not used in the provided code.\n",
    "\n",
    "        if handle_emojis == 'replace':\n",
    "            # Convert emojis to text representation with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_demojize_text)\n",
    "        elif handle_emojis == 'remove':\n",
    "            # Remove emojis with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_remove_emoji)\n",
    "\n",
    "        if spell_correction:\n",
    "            # Import TextBlob only if spell correction is enabled to avoid unnecessary import\n",
    "            from textblob import TextBlob\n",
    "            # Apply spell correction with a progress bar. This can be very slow.\n",
    "            processed_df[col] = processed_df[col].progress_apply(lambda x: str(TextBlob(x).correct()))\n",
    "        if remove_punc:\n",
    "            # Apply punctuation removal with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_remove_punc)\n",
    "        if remove_stopwords:\n",
    "            # Apply stopword removal with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_remove_stopwords)\n",
    "\n",
    "        if root_word_reduction == 'lemmatize':\n",
    "            if nlp: # Only attempt lemmatization if spaCy model was loaded successfully\n",
    "                # Apply lemmatization with a progress bar\n",
    "                processed_df[col] = processed_df[col].progress_apply(_lemmatize_text)\n",
    "            else:\n",
    "                print(f\"Skipping lemmatization for column '{col}' as spaCy model was not loaded.\")\n",
    "        elif root_word_reduction == 'stem':\n",
    "            # Apply stemming with a progress bar\n",
    "            processed_df[col] = processed_df[col].progress_apply(_stem_words)\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d982d4-9d33-4ad9-98d0-64023b2bc71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = preprocess_text_data(df, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468c686-6ffb-42b2-9bb6-c9a2c37666b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e7701-ce89-4477-b830-ecf002363d25",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf22ab-157e-4740-8fef-ffd438fc2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Features\n",
    "\n",
    "new_df['q1_len'] = new_df['question1'].str.len() \n",
    "new_df['q2_len'] = new_df['question2'].str.len()\n",
    "\n",
    "new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "new_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "\n",
    "def common_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "    return len(w1 & w2)\n",
    "new_df['word_common'] = new_df.apply(common_words, axis=1)\n",
    "\n",
    "def total_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "    return (len(w1) + len(w2))\n",
    "new_df['word_total'] = new_df.apply(total_words, axis=1)\n",
    "\n",
    "new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357df951-55e9-42d5-a6f5-2adf1f07eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Features (Part 1)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def fetch_token_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    SAFE_DIV = 0.0001 \n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features\n",
    "\n",
    "token_features = new_df.apply(fetch_token_features, axis=1)\n",
    "\n",
    "new_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "new_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "new_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "new_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "new_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "new_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "new_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "new_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445ba31-7410-454a-a5a2-1ad28a28ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Advanced Features (Part 2)\n",
    "\n",
    "import distance\n",
    "\n",
    "def fetch_length_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    length_features = [0.0]*3\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "    \n",
    "    # Absolute length features\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    \n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    \n",
    "    return length_features\n",
    "\n",
    "length_features = new_df.apply(fetch_length_features, axis=1)\n",
    "\n",
    "new_df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\n",
    "new_df['mean_len'] = list(map(lambda x: x[1], length_features))\n",
    "new_df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde6239-664b-4a51-8d46-8f80c2e5e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Features (Fuzzy Features)\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzy_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features\n",
    "\n",
    "fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n",
    "\n",
    "# Creating new feature columns for fuzzy features\n",
    "new_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "new_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "new_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "new_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))\n",
    "\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b547e3-3d9a-48c4-b0ad-4d86642aa96d",
   "metadata": {},
   "source": [
    "# Count Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c3f14-789b-4a3b-82de-3e55faa976d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_df = new_df[['question1','question2']]\n",
    "ques_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba3ee5-77ad-45e2-9337-4d8837b93de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ea661-117f-4211-b29d-527458bbffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# merge texts\n",
    "questions = list(ques_df['question1']) + list(ques_df['question2'])\n",
    "\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "q1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956255d-ed75-4444-bbc7-4581c49e6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\n",
    "temp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\n",
    "temp_df = pd.concat([temp_df1, temp_df2], axis=1)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f0caa-c90c-4438-b868-ff0b67db68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_df, temp_df], axis=1)\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f881c50-5978-4ffd-a1a5-5f7cf5910f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm # Import tqdm for the progress bar\n",
    "\n",
    "# Merge all questions (question1 and question2) into a single list\n",
    "# This is done to fit the CountVectorizer on the full vocabulary\n",
    "questions = list(ques_df['question1']) + list(ques_df['question2'])\n",
    "\n",
    "# Initialize CountVectorizer with a maximum of 3000 features (most frequent words)\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "\n",
    "# We will fit and transform the data in a loop to show progress.\n",
    "# First, fit the vectorizer on the entire corpus to build the vocabulary.\n",
    "print(\"Fitting the CountVectorizer...\")\n",
    "cv.fit(questions)\n",
    "\n",
    "# Get the number of questions in question1 to split the sparse matrix\n",
    "num_q1 = len(ques_df['question1'])\n",
    "\n",
    "# Now, transform the data in batches to show a progress bar\n",
    "# This is a more advanced technique for very large datasets,\n",
    "# but it illustrates how to use tqdm with a generator.\n",
    "# For simplicity, we'll create the full list of questions and then transform them.\n",
    "print(\"Transforming text to a sparse matrix...\")\n",
    "sparse_matrix = []\n",
    "for i in tqdm(range(len(questions)), desc=\"Processing questions\"):\n",
    "    sparse_matrix.append(cv.transform([questions[i]]))\n",
    "\n",
    "# Use vstack from scipy.sparse to combine the list of sparse matrices\n",
    "# into a single sparse matrix. This is more memory-efficient than\n",
    "# creating a very large list and then trying to combine it all at once.\n",
    "from scipy.sparse import vstack\n",
    "sparse_matrix = vstack(sparse_matrix)\n",
    "\n",
    "# Split the single sparse matrix back into two separate sparse matrices\n",
    "# for question1 and question2.\n",
    "q1_sparse_arr = sparse_matrix[:num_q1]\n",
    "q2_sparse_arr = sparse_matrix[num_q1:]\n",
    "\n",
    "# The resulting q1_sparse_arr and q2_sparse_arr are both sparse matrices\n",
    "# which are highly memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514011e-5923-4eab-b107-ed8577db0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming q1_sparse_arr and q2_sparse_arr are your sparse matrices from the previous step.\n",
    "\n",
    "# Concatenate the two sparse matrices horizontally (side-by-side)\n",
    "# This results in one row per question pair, with features from both questions.\n",
    "# The resulting matrix is still sparse and highly memory-efficient.\n",
    "feature_matrix = hstack([q1_sparse_arr, q2_sparse_arr])\n",
    "\n",
    "# Print the shape of the final feature matrix\n",
    "# The number of rows will be the number of question pairs.\n",
    "# The number of columns will be the sum of the features from both sparse matrices (3000 + 3000 = 6000 in your case).\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ae7ca-b7f6-4113-a3b6-a3e7e591765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Assuming final_df contains your engineered features and is a dense DataFrame.\n",
    "# Assuming feature_matrix is the sparse BoW matrix from the previous step.\n",
    "\n",
    "# 1. Convert the final_df (engineered features) to a sparse matrix\n",
    "# This is a crucial step to handle the data consistently and efficiently.\n",
    "engineered_features_sparse = csr_matrix(final_df)\n",
    "\n",
    "# 2. Horizontally stack the engineered features sparse matrix with the BoW sparse matrix\n",
    "# The result is a single, unified sparse matrix that combines all features.\n",
    "combined_features_sparse = hstack([engineered_features_sparse, feature_matrix])\n",
    "\n",
    "# Print the shape of the new combined sparse matrix\n",
    "# It should now have:\n",
    "#   - original engineered features columns (e.g., 20)\n",
    "#   - 6000 BoW features (3000 from question1 and 3000 from question2)\n",
    "# Total columns = 20 + 6000 = 6020 (example)\n",
    "print(combined_features_sparse.shape)\n",
    "\n",
    "# You can now use combined_features_sparse for your model training.\n",
    "# Most scikit-learn models are optimized to work directly with sparse matrices.\n",
    "\n",
    "# Note: We do not call .head() because the output is a sparse matrix, not a DataFrame.\n",
    "# To inspect the data, you would need to convert it to an array or DataFrame,\n",
    "# which is not recommended for large datasets due to memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587213c-97a6-4e49-8061-0624ea1b0a5a",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2dd0d-8824-4079-a868-ce17768f1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "\n",
    "# X = all feature columns (excluding the first column, which is the target label 'is_duplicate')\n",
    "X = final_df.iloc[:, 1:].values\n",
    "\n",
    "# y = the target column (first column, 'is_duplicate')\n",
    "y = final_df.iloc[:, 0].values\n",
    "\n",
    "# Perform 80/20 train-test split\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9f06b-ff17-4cf3-af03-092ef16b4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Total number of estimators (trees) you want in your final forest\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "# Set n_estimators to 1 to start, and use warm_start=True\n",
    "# warm_start=True allows us to add more trees incrementally\n",
    "rf = RandomForestClassifier(n_estimators=1, warm_start=True, random_state=42)\n",
    "\n",
    "# Create an empty list to store the accuracy at each step, if you want to track it\n",
    "accuracies = []\n",
    "\n",
    "print(\"Training Random Forest with a progress bar...\")\n",
    "\n",
    "# Loop to incrementally add trees to the forest and show a progress bar\n",
    "# We will add trees in batches of 10 to speed up the process.\n",
    "# tqdm will display the progress of this loop.\n",
    "with tqdm(total=N_ESTIMATORS, desc=\"Building Random Forest\") as pbar:\n",
    "    for i in range(1, N_ESTIMATORS + 1):\n",
    "        # Set n_estimators to the current iteration number\n",
    "        # This adds one more tree at each iteration.\n",
    "        rf.n_estimators = i\n",
    "        \n",
    "        # Fit the model with the current number of trees\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Now that the model is fully trained, we can use it for predictions.\n",
    "print(\"Random Forest training complete.\")\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate and return the accuracy of the model on test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the model on test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2756311-9a94-45fc-943f-37d1caf838ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the following variables from the previous steps:\n",
    "# y_test: The true labels for the test data\n",
    "# y_pred: The model's predictions for the test data\n",
    "\n",
    "# 1. Calculate and display the overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# 2. Generate and display the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# For better readability, you can display the confusion matrix in a DataFrame\n",
    "conf_df = pd.DataFrame(conf_matrix, \n",
    "                       index=['Actual Negative', 'Actual Positive'], \n",
    "                       columns=['Predicted Negative', 'Predicted Positive'])\n",
    "print(\"\\nConfusion Matrix (DataFrame):\")\n",
    "print(conf_df)\n",
    "\n",
    "# 3. Generate and display the classification report for more detailed metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d96df7-2a8e-4d71-b34c-da7914d978f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(rf,open('model_final.pkl','wb'))\n",
    "pickle.dump(cv,open('cv_final.pkl','wb'))\n",
    "\n",
    "print(\"Vectorizer and model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee9e75-287d-439a-8e95-df424b067b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04f59a-b824-4968-bbe0-3bba97d84e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932cefe-1e32-4640-885d-b7b09a192d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ffa31-0816-4788-810b-2e00ee33399e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
